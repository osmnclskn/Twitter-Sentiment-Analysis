{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.6.tar.gz (84 kB)\n",
      "     ---------------------------------------- 0.0/84.6 kB ? eta -:--:--\n",
      "     -------------- ------------------------- 30.7/84.6 kB 1.3 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 41.0/84.6 kB 495.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- 84.6/84.6 kB 790.6 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from kaggle) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from kaggle) (4.66.1)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from kaggle) (2.1.0)\n",
      "Requirement already satisfied: bleach in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.2/78.2 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from requests->kaggle) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (pyproject.toml): started\n",
      "  Building wheel for kaggle (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.6-py3-none-any.whl size=111953 sha256=6312f727f6117fc7127ae3bd7ab05a36ae93d309fff1bcbd2a1c8b92b159f2e2\n",
      "  Stored in directory: c:\\users\\osmnc\\appdata\\local\\pip\\cache\\wheels\\53\\34\\8c\\8ca3450d17206d9e37e1ee3aeb47cbb2873d22a9e0c60eb137\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.6.6 python-slugify-8.0.4 text-unidecode-1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Klasör oluşturma\n",
    "os.makedirs(os.path.join(os.path.expanduser(\"~\"), \".kaggle\"), exist_ok=True)\n",
    "\n",
    "# Dosya kopyalama\n",
    "shutil.copy(\"kaggle.json\", os.path.join(os.path.expanduser(\"~\"), \".kaggle\"))\n",
    "\n",
    "# Dosya izinlerini değiştirme\n",
    "os.chmod(os.path.join(os.path.expanduser(\"~\"), \".kaggle\", \"kaggle.json\"), 0o600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sentiment140.zip to c:\\Users\\osmnc\\entrapeer-task\\venv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/80.9M [00:00<?, ?B/s]\n",
      "  1%|          | 1.00M/80.9M [00:00<01:01, 1.36MB/s]\n",
      "  2%|▏         | 2.00M/80.9M [00:00<00:30, 2.71MB/s]\n",
      "  5%|▍         | 4.00M/80.9M [00:01<00:13, 5.81MB/s]\n",
      "  9%|▊         | 7.00M/80.9M [00:01<00:07, 10.2MB/s]\n",
      " 11%|█         | 9.00M/80.9M [00:01<00:06, 12.0MB/s]\n",
      " 15%|█▍        | 12.0M/80.9M [00:01<00:04, 15.8MB/s]\n",
      " 19%|█▊        | 15.0M/80.9M [00:01<00:03, 18.5MB/s]\n",
      " 22%|██▏       | 18.0M/80.9M [00:01<00:03, 21.1MB/s]\n",
      " 26%|██▌       | 21.0M/80.9M [00:01<00:02, 22.8MB/s]\n",
      " 30%|██▉       | 24.0M/80.9M [00:01<00:02, 24.3MB/s]\n",
      " 33%|███▎      | 27.0M/80.9M [00:01<00:02, 25.1MB/s]\n",
      " 37%|███▋      | 30.0M/80.9M [00:02<00:02, 25.9MB/s]\n",
      " 41%|████      | 33.0M/80.9M [00:02<00:01, 26.4MB/s]\n",
      " 44%|████▍     | 36.0M/80.9M [00:02<00:01, 27.0MB/s]\n",
      " 48%|████▊     | 39.0M/80.9M [00:02<00:01, 27.2MB/s]\n",
      " 52%|█████▏    | 42.0M/80.9M [00:02<00:01, 27.4MB/s]\n",
      " 56%|█████▌    | 45.0M/80.9M [00:02<00:01, 26.5MB/s]\n",
      " 59%|█████▉    | 48.0M/80.9M [00:02<00:01, 27.2MB/s]\n",
      " 63%|██████▎   | 51.0M/80.9M [00:02<00:01, 27.1MB/s]\n",
      " 67%|██████▋   | 54.0M/80.9M [00:02<00:01, 26.6MB/s]\n",
      " 70%|███████   | 57.0M/80.9M [00:03<00:00, 26.7MB/s]\n",
      " 74%|███████▍  | 60.0M/80.9M [00:03<00:00, 27.0MB/s]\n",
      " 78%|███████▊  | 63.0M/80.9M [00:03<00:00, 27.1MB/s]\n",
      " 82%|████████▏ | 66.0M/80.9M [00:03<00:00, 27.4MB/s]\n",
      " 85%|████████▌ | 69.0M/80.9M [00:03<00:00, 28.4MB/s]\n",
      " 89%|████████▉ | 72.0M/80.9M [00:03<00:00, 27.4MB/s]\n",
      " 93%|█████████▎| 75.0M/80.9M [00:03<00:00, 27.6MB/s]\n",
      " 96%|█████████▋| 78.0M/80.9M [00:03<00:00, 27.8MB/s]\n",
      "100%|██████████| 80.9M/80.9M [00:04<00:00, 27.8MB/s]\n",
      "100%|██████████| 80.9M/80.9M [00:04<00:00, 21.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Get dataset with Kaggle API\n",
    "!kaggle datasets download -d kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is extracted\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "data = \"C:\\\\Users\\\\osmnc\\\\entrapeer-task\\\\venv\\\\sentiment140.zip\"\n",
    "\n",
    "with ZipFile(data,'r') as zip:\n",
    "    zip.extractall()\n",
    "    print('Dataset is extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\osmnc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv('C:\\\\Users\\\\osmnc\\\\entrapeer-task\\\\venv\\\\training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=['target','ids','date','flag','user','text']\n",
    "twitter_data = pd.read_csv('C:\\\\Users\\\\osmnc\\\\entrapeer-task\\\\venv\\\\training.1600000.processed.noemoticon.csv',names=column_names, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "ids       0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking missing values in df\n",
    "twitter_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the distribution of the target column\n",
    "twitter_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.replace({'target':{4:1}},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting swifter\n",
      "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.0/1.2 MB 1.3 MB/s eta 0:00:01\n",
      "     - -------------------------------------- 0.0/1.2 MB 1.3 MB/s eta 0:00:01\n",
      "     -- ------------------------------------- 0.1/1.2 MB 651.6 kB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.1/1.2 MB 819.2 kB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.2/1.2 MB 980.4 kB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.2/1.2 MB 1.2 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.5/1.2 MB 1.6 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 0.5/1.2 MB 1.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.0/1.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.2/1.2 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 2.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from swifter) (2.1.4)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from swifter) (5.9.7)\n",
      "Collecting dask>=2.10.0 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading dask-2024.2.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from swifter) (4.66.1)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
      "Collecting cloudpickle>=1.5.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (23.2)\n",
      "Collecting partd>=1.2.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading partd-1.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.1)\n",
      "Collecting toolz>=0.10.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting importlib-metadata>=4.13.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from pandas>=1.0.0->swifter) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from pandas>=1.0.0->swifter) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from pandas>=1.0.0->swifter) (2023.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from tqdm>=4.33.0->swifter) (0.4.6)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting locket (from partd>=1.2.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\osmnc\\entrapeer-task\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.16.0)\n",
      "Downloading dask-2024.2.1-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 1.0/1.2 MB 64.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.0/1.2 MB 64.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.2/1.2 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 9.8 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading partd-1.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.1/56.1 kB ? eta 0:00:00\n",
      "Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Building wheels for collected packages: swifter\n",
      "  Building wheel for swifter (pyproject.toml): started\n",
      "  Building wheel for swifter (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16512 sha256=52451fa36b62ce3886514ad6c1f80ddebb3c0d7bec50060415158a6b7627d712\n",
      "  Stored in directory: c:\\users\\osmnc\\appdata\\local\\pip\\cache\\wheels\\e4\\cf\\51\\0904952972ee2c7aa3709437065278dc534ec1b8d2ad41b443\n",
      "Successfully built swifter\n",
      "Installing collected packages: zipp, toolz, locket, cloudpickle, partd, importlib-metadata, dask, swifter\n",
      "Successfully installed cloudpickle-3.0.0 dask-2024.2.1 importlib-metadata-7.0.1 locket-1.0.0 partd-1.4.1 swifter-1.4.0 toolz-0.12.1 zipp-3.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stemming_snowball(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z^]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [snowball_stemmer.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "\n",
    "    return stemmed_content\n",
    "\n",
    "  \n",
    "twitter_data['stemmed_content']=twitter_data['text'].apply(stemming_snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>stemmed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>switchfoot http twitpic com zl awww bummer sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset updat facebook text might cri result sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>kenichan dive mani time ball manag save rest g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass behav mad see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     stemmed_content  \n",
       "0  switchfoot http twitpic com zl awww bummer sho...  \n",
       "1  upset updat facebook text might cri result sch...  \n",
       "2  kenichan dive mani time ball manag save rest g...  \n",
       "3                    whole bodi feel itchi like fire  \n",
       "4                      nationwideclass behav mad see  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          switchfoot http twitpic com zl awww bummer sho...\n",
      "1          upset updat facebook text might cri result sch...\n",
      "2          kenichan dive mani time ball manag save rest g...\n",
      "3                            whole bodi feel itchi like fire\n",
      "4                              nationwideclass behav mad see\n",
      "                                 ...                        \n",
      "1599995                           woke school best feel ever\n",
      "1599996    thewdb com cool hear old walt interview http b...\n",
      "1599997                         readi mojo makeov ask detail\n",
      "1599998    happi th birthday boo alll time tupac amaru sh...\n",
      "1599999    happi charitytuesday thenspcc sparkschar speak...\n",
      "Name: stemmed_content, Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(twitter_data['stemmed_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0\n",
      "1          0\n",
      "2          0\n",
      "3          0\n",
      "4          0\n",
      "          ..\n",
      "1599995    1\n",
      "1599996    1\n",
      "1599997    1\n",
      "1599998    1\n",
      "1599999    1\n",
      "Name: target, Length: 1600000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(twitter_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating the data and label\n",
    "X=twitter_data['stemmed_content'].values\n",
    "y=twitter_data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['switchfoot http twitpic com zl awww bummer shoulda got david carr third day'\n",
      " 'upset updat facebook text might cri result school today also blah'\n",
      " 'kenichan dive mani time ball manag save rest go bound' ...\n",
      " 'readi mojo makeov ask detail'\n",
      " 'happi th birthday boo alll time tupac amaru shakur'\n",
      " 'happi charitytuesday thenspcc sparkschar speakinguph h']\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y,random_state=58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love spend age fuck bunch motorcycl chain enough time afternoon poo'\n",
      " 'enjoy sunday minus cold wind'\n",
      " 'x mrspattinson oh right kk im beddd talk tomorrow goodnight lt' ...\n",
      " 'somewhat sick' 'mathijsvdhurk succ meneer'\n",
      " 'last titan survivor die today wonder next generat know titan tragedi movi']\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['want peopl know still bore haha' 'got confirm mres' 'thank' ...\n",
      " 'kutski quit piss wait upgrad get new handset though'\n",
      " 'mileycyrus hi miley want ask inspir becom actor pleas repli'\n",
      " 'ohhhhh line']\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the text data to numerical data \n",
    "\n",
    "vectorizer=TfidfVectorizer()\n",
    "#fit_transform is used in the training dataset\n",
    "X_train=vectorizer.fit_transform(X_train)\n",
    "X_test=vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 157658)\t0.4110959222701648\n",
      "  (0, 48541)\t0.471328056304594\n",
      "  (0, 385054)\t0.3831773126902061\n",
      "  (0, 220773)\t0.3654789610133353\n",
      "  (0, 312893)\t0.44455760355296664\n",
      "  (0, 436609)\t0.361688843140543\n",
      "  (1, 276506)\t0.8194619643192655\n",
      "  (1, 79784)\t0.5124372546410653\n",
      "  (1, 151912)\t0.2566907654940837\n",
      "  (2, 401004)\t1.0\n",
      "  (3, 386011)\t0.37870553365765136\n",
      "  (3, 302225)\t0.13839406267269228\n",
      "  (3, 447220)\t0.20457703997970858\n",
      "  (3, 370118)\t0.21537831642373184\n",
      "  (3, 438210)\t0.24274402945605075\n",
      "  (3, 315663)\t0.4545614567291185\n",
      "  (3, 109377)\t0.33049227361666794\n",
      "  (3, 92418)\t0.32226490587475415\n",
      "  (3, 399184)\t0.3292635996868391\n",
      "  (3, 150921)\t0.12296918227353076\n",
      "  (3, 132303)\t0.178453381060559\n",
      "  (3, 1811)\t0.2116404354872998\n",
      "  (3, 38129)\t0.26530322736379985\n",
      "  (4, 6289)\t0.4357441348851245\n",
      "  (4, 197313)\t0.9000705799620464\n",
      "  :\t:\n",
      "  (1279996, 436543)\t0.3828161106444139\n",
      "  (1279996, 149923)\t0.23998677251133627\n",
      "  (1279996, 31318)\t0.3453132636550597\n",
      "  (1279996, 267966)\t0.2972706456804465\n",
      "  (1279997, 224481)\t0.5318020674740658\n",
      "  (1279997, 158941)\t0.5021306689174122\n",
      "  (1279997, 426831)\t0.34976330226984376\n",
      "  (1279997, 317624)\t0.3201354286580218\n",
      "  (1279997, 330135)\t0.2848310734278135\n",
      "  (1279997, 407649)\t0.2227703529906325\n",
      "  (1279997, 435916)\t0.2105228422067842\n",
      "  (1279997, 146075)\t0.1589128771335589\n",
      "  (1279997, 288655)\t0.1997542287282055\n",
      "  (1279998, 3220)\t0.4217125706921187\n",
      "  (1279998, 36539)\t0.34937117587579314\n",
      "  (1279998, 181119)\t0.3586482603505074\n",
      "  (1279998, 266183)\t0.3381213446387852\n",
      "  (1279998, 266198)\t0.31203992846600237\n",
      "  (1279998, 338563)\t0.30036580376524447\n",
      "  (1279998, 165807)\t0.2804227677533177\n",
      "  (1279998, 318844)\t0.2559526357032358\n",
      "  (1279998, 25602)\t0.2929360684135629\n",
      "  (1279998, 436609)\t0.19898471607275695\n",
      "  (1279999, 299821)\t0.8236334549000647\n",
      "  (1279999, 236274)\t0.567122501730784\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 409458)\t0.17347016304114848\n",
      "  (0, 379080)\t0.2844044603409499\n",
      "  (0, 320771)\t0.36217349048340064\n",
      "  (0, 275531)\t0.40248383323645065\n",
      "  (0, 242266)\t0.1648630684735437\n",
      "  (0, 140066)\t0.2531065207876272\n",
      "  (0, 120637)\t0.2658092885076052\n",
      "  (0, 66355)\t0.3869089731207089\n",
      "  (0, 55978)\t0.3318344283544054\n",
      "  (0, 5897)\t0.30478081559985853\n",
      "  (0, 5656)\t0.2850579332311813\n",
      "  (1, 443496)\t0.5161871801466902\n",
      "  (1, 389142)\t0.381148756942759\n",
      "  (1, 267319)\t0.5612422581548667\n",
      "  (1, 120509)\t0.35516240567800295\n",
      "  (1, 78058)\t0.383592509555958\n",
      "  (2, 412903)\t0.1997875851734987\n",
      "  (2, 394741)\t0.23507148914250453\n",
      "  (2, 341357)\t0.20812398792739756\n",
      "  (2, 299565)\t0.19404585804943808\n",
      "  (2, 277411)\t0.5297891632466966\n",
      "  (2, 243703)\t0.22829216586115897\n",
      "  (2, 219542)\t0.39705263465631\n",
      "  (2, 178087)\t0.18535100973352817\n",
      "  (2, 151120)\t0.27497407951042524\n",
      "  :\t:\n",
      "  (319995, 15007)\t0.21700351213850091\n",
      "  (319996, 420701)\t0.22420293738958202\n",
      "  (319996, 384977)\t0.42736438093524937\n",
      "  (319996, 378818)\t0.3191765809778941\n",
      "  (319996, 336061)\t0.33476331070730675\n",
      "  (319996, 275767)\t0.16689114135474165\n",
      "  (319996, 266405)\t0.35743061035871626\n",
      "  (319996, 228066)\t0.42736438093524937\n",
      "  (319996, 166001)\t0.2054514612833748\n",
      "  (319996, 164986)\t0.16213094751802026\n",
      "  (319996, 18273)\t0.38263064340226655\n",
      "  (319997, 376148)\t0.8593291134999056\n",
      "  (319997, 367559)\t0.5114229899911291\n",
      "  (319998, 387759)\t1.0\n",
      "  (319999, 445364)\t0.21111834515461517\n",
      "  (319999, 415448)\t0.3490024073258359\n",
      "  (319999, 411890)\t0.1451724996396653\n",
      "  (319999, 410642)\t0.6438962410683485\n",
      "  (319999, 390830)\t0.3417273345621714\n",
      "  (319999, 289067)\t0.1907425649857985\n",
      "  (319999, 275767)\t0.1954675448067501\n",
      "  (319999, 228207)\t0.1673025665567833\n",
      "  (319999, 220773)\t0.153552169505872\n",
      "  (319999, 145175)\t0.3280314344870539\n",
      "  (319999, 100729)\t0.21773387931413568\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "\n",
    "model=LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Score\n",
    "X_train_predic=model.predict(X_train)\n",
    "training_data_acc=accuracy_score(y_train,X_train_predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score on the training data :  0.8105484375\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score on the training data : ',training_data_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predic=model.predict(X_test)\n",
    "test_data_acc=accuracy_score(y_test,X_test_predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the Test data :  0.777553125\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score on the Test data : ',test_data_acc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='trained_model.sav'\n",
    "pickle.dump(model,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the saved model for future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the saved model\n",
    "\n",
    "loaded_model = pickle.load(open('C:\\\\Users\\\\osmnc\\\\entrapeer-task\\\\venv\\\\trained_model.sav', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1]\n",
      "Positive Tweet\n"
     ]
    }
   ],
   "source": [
    "X_new=X_test[300]\n",
    "print(y_test[300])\n",
    "\n",
    "\n",
    "prediction=loaded_model.predict(X_new)\n",
    "print(prediction)\n",
    "\n",
    "if prediction[0]==0:\n",
    "    print('Negative Tweet')\n",
    "\n",
    "else:\n",
    "    print('Positive Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0]\n",
      "Negative Tweet\n"
     ]
    }
   ],
   "source": [
    "X_new=X_test[3]\n",
    "print(y_test[3])\n",
    "\n",
    "\n",
    "prediction=loaded_model.predict(X_new)\n",
    "print(prediction)\n",
    "\n",
    "if prediction[0]==0:\n",
    "    print('Negative Tweet')\n",
    "\n",
    "else:\n",
    "    print('Positive Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
